{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236e1ac7-92c8-4429-b186-5cf51b36f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbb2d52-477a-46f8-98c1-3457530c095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastvit import fastvit_t8\n",
    "# keras_model = fastvit_t8()\n",
    "# keras_model.summary(expand_nested=True)\n",
    "# keras_model.load_weights(\"fastvit_t8_patch_embed.weights.h5\", skip_mismatch=False)\n",
    "# # for i in keras_model.variables:\n",
    "# #     print(i.path, i.value.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aac0028-5b7a-4cdb-b2ba-d5f4f3ec1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pt_fastvit import fastvit_t8 as pt_fastvit_t8\n",
    "# from torchinfo import summary\n",
    "\n",
    "# pt_model = pt_fastvit_t8(inference_mode=False) \n",
    "\n",
    "# pt_model.load_state_dict(torch.load(\"pt_fastvit_t8_patch_embed.pth\", weights_only=True), strict=False)\n",
    "# # for k, v in pt_model.state_dict().items():\n",
    "# #     print(k, v.sum())\n",
    "\n",
    "# summary(pt_model, (1, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e40883e-c59d-47c6-bd9c-1780861649a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\miniconda3\\envs\\keras_univ\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pt_fastvit import fastvit_t8 as pt_fastvit_t8\n",
    "from pt_fastvit import fastvit_t12 as pt_fastvit_t12\n",
    "from pt_fastvit import fastvit_ma36 as pt_fastvit_ma36\n",
    "from torchinfo import summary\n",
    "\n",
    "pt_model = pt_fastvit_ma36(inference_mode=False) \n",
    "\n",
    "pt_model.load_state_dict(torch.load(\"fastvit_ma36.pth.tar\", weights_only=True)[\"state_dict\"], strict=True)\n",
    "# pt_model.load_state_dict(torch.load(\"pt_fastvit_t8_patch_embed.pth\", weights_only=True), strict=False)\n",
    "# for k, v in pt_model.state_dict().items():\n",
    "#     print(k, v.sum())\n",
    "\n",
    "# summary(pt_model, (1, 3, 256, 256))\n",
    "# pt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc52474b-17b8-4df1-a49c-8fb3e29724b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-59884.4219) torch.Size([1, 3, 256, 256])\n",
      "preds.shape torch.Size([1, 1000])\n",
      "preds.sum() -2381.94921875\n",
      "preds.sum() 388\n",
      "--------\n",
      "Output: torch.Size([1, 1216, 8, 8]) tensor(6865.1826)\n"
     ]
    }
   ],
   "source": [
    "# ImageNet normalization constants for PyTorch (tensors)\n",
    "pt_IMAGENET_DEFAULT_MEAN = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
    "pt_IMAGENET_DEFAULT_STD = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
    "\n",
    "activations = {}\n",
    "\n",
    "def capture_output(module, input, output):\n",
    "    # print(str(module))\n",
    "    activations[\"input\"] = input[0].detach()\n",
    "    activations['output'] = output.detach()\n",
    "\n",
    "\n",
    "pt_model.conv_exp.register_forward_hook(capture_output)\n",
    "\n",
    "def test_prediction_pytorch(*, image_path, model, image_shape=(224, 224), show=False):\n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    img = cv2.resize(img, image_shape, interpolation=cv2.INTER_CUBIC)  # Resize to desired shape\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Convert the image to a PyTorch tensor and rearrange dimensions: HWC -> CHW\n",
    "    img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
    "\n",
    "    # Apply ImageNet normalization: (input - mean) / std\n",
    "    # The mean and std tensors are reshaped to broadcast across height and width\n",
    "    img_tensor = (img_tensor - pt_IMAGENET_DEFAULT_MEAN[:, None, None]) / pt_IMAGENET_DEFAULT_STD[:, None, None]\n",
    "\n",
    "    # Add a batch dimension: shape becomes (1, C, H, W)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    print(img_tensor.sum(), img_tensor.shape)\n",
    "\n",
    "    # Ensure the model is in evaluation mode and perform inference without gradients\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(img_tensor)\n",
    "\n",
    "    print(\"preds.shape\", preds.shape)\n",
    "    print(\"preds.sum()\", preds.sum().item())\n",
    "    print(\"preds.sum()\", preds.argmax(dim=1).item())\n",
    "\n",
    "    # # Get the index of the highest score (predicted class)\n",
    "    predicted_class = preds.argmax(dim=1).item()\n",
    "    # print(f\"Model: {model.__class__.__name__}, Predictions: {predicted_class}\")\n",
    "\n",
    "\n",
    "test_prediction_pytorch(image_path=r\"C:\\Users\\vaibh\\OneDrive\\Desktop\\Work\\other_work_mine\\Mine\\keras-vision\\test_images\\pandas.JPG\", model=pt_model, image_shape=(256, 256))\n",
    "\n",
    "\n",
    "# print(\"--------\")\n",
    "# inputx = activations['input']\n",
    "# print(\"input:\", inputx.shape, inputx.sum())\n",
    "\n",
    "print(\"--------\")\n",
    "# Retrieve the output from the SiLU activation layer\n",
    "output = activations['output']\n",
    "print(\"Output:\", output.shape, output.sum())\n",
    "\n",
    "# print(logits.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff76367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d712d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8aa90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286e5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_univ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
